{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from functools import partial\n",
    "import json\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from gerrychain import (\n",
    "    Election,\n",
    "    Graph,\n",
    "    MarkovChain,\n",
    "    Partition,\n",
    "    accept,\n",
    "    constraints,\n",
    "    updaters,\n",
    ")\n",
    "from gerrychain.metrics import efficiency_gap, mean_median, polsby_popper\n",
    "from gerrychain.proposals import recom, flip\n",
    "from gerrychain.updaters import cut_edges\n",
    "from gerrychain.tree import recursive_tree_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label = \"GEOID10\"\n",
    "pop_col = \"TOTPOP\"\n",
    "district_col = \"CD\"\n",
    "\n",
    "plot_path = \"../DiffusionDistances/Data/PA_VTD.shp\"\n",
    "graph = Graph.from_json(\"../DiffusionDistances/Data/PA_VTD.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "updaters = {\n",
    "    \"population\": updaters.Tally(\"TOT_POP\", alias=\"population\"),\n",
    "    \"cut_edges\": cut_edges, #\"black_pop\": updaters.Tally(\"BPOP\", alias = \"black_pop\"),\n",
    "    #\"nh_white\": updaters.Tally(\"NH_WHITE\", alias = \"nh_white\"),\n",
    "    #\"nh_black\": updaters.Tally(\"NH_BLACK\", alias = \"nh_black\"),\n",
    "    #\"hisp\": updaters.Tally(\"HISP\", alias = \"hisp\"),\n",
    "    #\"vap\": updaters.Tally(\"VAP\", alias = \"vap\"),\n",
    "    #\"hvap\": updaters.Tally(\"HVAP\", alias = \"hvap\"),\n",
    "    #\"wvap\": updaters.Tally(\"WVAP\", alias = \"wvap\"),\n",
    "    #\"bvap\":updaters.Tally(\"BVAP\", alias = \"bvap\")\n",
    "}\n",
    "\n",
    "initial_partition = Partition(graph, \"GOV\", updaters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dist = len(initial_partition)\n",
    "ideal_population = sum(initial_partition[\"population\"].values()) / len(\n",
    "    initial_partition)\n",
    "random_plan = recursive_tree_part(graph, range(num_dist), ideal_population, \"TOT_POP\", 0.01, 1)\n",
    "base_partition = Partition(graph, random_plan, updaters)\n",
    "random_plan_2 = recursive_tree_part(graph, range(num_dist), ideal_population, \"TOT_POP\", 0.01, 1)\n",
    "new_partition = Partition(graph, random_plan_2, updaters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a base_partition with some labelling and a new_partition, we would like \n",
    "# the labelling of new_partition to be as close as possible to the base_partition.\n",
    "# That is, we would like as many nodes as possible to remain in the same district.\n",
    "# This function implements a greedy algorithm to do so (thus is not necessarily optimal).\n",
    "# It outputs a dictionary that takes the labelling given by base_partition and \n",
    "# says which label in new_partition should be sent to that base_partition label.  \n",
    "# It also outputs the number of displaced nodes\n",
    "# (so the output is a tuple with first entry the dictionary, second entry number of\n",
    "# displaced nodes)\n",
    "\n",
    "def greedy_hamming(base_partition, new_partition):\n",
    "    names = [j for j in base_partition.parts]\n",
    "    new_names = {}\n",
    "    for i in new_partition.parts:\n",
    "        intersection_sizes = {}\n",
    "        for name in names:\n",
    "            intersection_sizes.update({len(set(base_partition.assignment.parts[name]).intersection(set(new_partition.assignment.parts[i]))): name})\n",
    "        new_names.update({intersection_sizes[max(intersection_sizes.keys())]: i})\n",
    "        names.remove(intersection_sizes[max(intersection_sizes.keys())])\n",
    "    tot_nodes = len(new_partition.assignment)\n",
    "    final_int_sizes = []\n",
    "    for i in base_partition.parts:\n",
    "        x = len(set(new_partition.assignment.parts[new_names[i]]).intersection(set(base_partition.assignment.parts[i])))\n",
    "        final_int_sizes.append(x)\n",
    "    ham_dist = tot_nodes - sum(final_int_sizes)\n",
    "    return new_names, ham_dist;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a base_partition with some labelling and a new_partition, we would like \n",
    "# the labelling of new_partition to be as close as possible to the base_partition.\n",
    "# That is, we would like as many people as possible to remain in the same district.\n",
    "# This function implements a greedy algorithm to do so (thus is not necessarily optimal).\n",
    "# It outputs a dictionary that takes the labelling given by base_partition and \n",
    "# says which label in new_partition should be sent to that base_partition label.  It also outputs the number of displaced people\n",
    "# (so the output is a tuple with first entry the dictionary, second entry number of\n",
    "# displaced people)\n",
    "\n",
    "\n",
    "def greedy_hamming_pop(base_partition, new_partition):\n",
    "    names = [j for j in base_partition.parts]\n",
    "    new_names = {}\n",
    "    for i in new_partition.parts:\n",
    "        intersections = {}\n",
    "        intersection_pops = {}\n",
    "        for name in names:\n",
    "            intersections.update({name: set(base_partition.assignment.parts[name]).intersection(set(new_partition.assignment.parts[i]))})\n",
    "            intersection_pops.update({sum([new_partition.graph.nodes[node][\"TOTPOP\"] for node in intersections[name]]): name})\n",
    "        new_names.update({intersection_pops[max(intersection_pops.keys())]: i})\n",
    "        names.remove(intersection_pops[max(intersection_pops.keys())])\n",
    "    tot_pop = sum(base_partition[\"population\"].values())\n",
    "    final_int_pops = []\n",
    "    for i in base_partition.parts:\n",
    "        intersection_set = set(new_partition.assignment.parts[new_names[i]]).intersection(set(base_partition.assignment.parts[i]))      \n",
    "        intersection_list = list(intersection_set)\n",
    "        intersection_pops = sum([new_partition.graph.nodes[node][\"TOTPOP\"] for node in intersection_list])\n",
    "        final_int_pops.append(intersection_pops)\n",
    "    ham_dist_pop = tot_pop - sum(final_int_pops)    \n",
    "    return new_names, ham_dist_pop;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_distance_pre_sym(base_partition, new_partition):\n",
    "    #build a graph and get dictionary from greedy_hamming algorithm of which districts correspond\n",
    "    graph = base_partition.graph\n",
    "    dist_list = list(base_partition.parts)\n",
    "    dist_dict = greedy_hamming(base_partition,new_partition)[0]\n",
    "    #This list holds the expected diffusion time for each district, averaged by\n",
    "    # the number of nodes in the district\n",
    "    diffusion_list = []\n",
    "    #iterating over all districts\n",
    "    for dist in dist_list:\n",
    "        #grabbing the corresponding district in the new_partition\n",
    "        dist2 = dist_dict[dist]\n",
    "        #build the random walk matrix for the graph\n",
    "        RW = (nx.adjacency_matrix(graph, weight = None)).astype(float)\n",
    "        for i in range(len(graph.nodes)):\n",
    "            RW[i,:] = RW[i,:]/len(list(graph.neighbors(i)))\n",
    "        #We delete entries from RW; this helps track indices\n",
    "        ref_list = list(graph.nodes)\n",
    "        ref_list = np.delete(ref_list,np.array(list(new_partition.parts[dist2])),0)\n",
    "        # Deleting these same entries from RW (by keeping the complement)\n",
    "        to_keep = sorted(list(set([i for i in range(len(graph.nodes))]).difference(set(new_partition.parts[dist2]))))\n",
    "        P = RW[to_keep,:][:,to_keep]\n",
    "        M = sp.sparse.identity(len(ref_list))\n",
    "        M = sp.sparse.csr_matrix(M)\n",
    "        M = M - P\n",
    "        uno = np.ones((len(ref_list),1))\n",
    "        rowsum = sp.sparse.linalg.spsolve(M, uno)\n",
    "        diff_nodes = list(set(base_partition.parts[dist]).difference(set(new_partition.parts[dist2])))\n",
    "        k = 0\n",
    "        for i in range(len(ref_list)):\n",
    "            if ref_list[i] in set(diff_nodes):\n",
    "                k += rowsum[i]\n",
    "        # Dividing that sum by the number of nodes in the sum\n",
    "        k = float(k)/len(diff_nodes)\n",
    "        diffusion_list.append(k)\n",
    "   \n",
    "    return diffusion_list\n",
    "\n",
    "# Function that measures the expected time it takes for random walks\n",
    "# between hamming paired districts to terminate.\n",
    "# Outputs a tuple.  First entry is a list that is the time (averaged over \n",
    "# number of nodes) in each district, second entry is the sum of those times.\n",
    "\n",
    "def diffusion_distance(base_partition,new_partition):\n",
    "    first_half = np.array(diffusion_distance_pre_sym(base_partition,new_partition))\n",
    "    second_half = np.array(diffusion_distance_pre_sym(new_partition,base_partition))\n",
    "    diffusion_list = first_half+second_half\n",
    "    # Final total diffusion time\n",
    "    total_diff_time = sum(diffusion_list)\n",
    "    return diffusion_list,total_diff_time;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
